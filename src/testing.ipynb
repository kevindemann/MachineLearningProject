{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from jax import random, nn\n",
    "from jax import grad, value_and_grad\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w_0': Array([[-0.62385625,  1.47832   , -0.3503256 ,  0.35584512],\n",
       "        [-0.6633469 ,  0.72462827,  0.64054126,  0.00546522],\n",
       "        [ 0.2012313 ,  0.94475996, -1.4384235 , -1.6024474 ]],      dtype=float32),\n",
       " 'b_0': Array([[ 1.0343399 , -0.19056194,  1.4303523 , -0.5464108 ]], dtype=float32),\n",
       " 'w_1': Array([[-0.07426444],\n",
       "        [-0.7407608 ],\n",
       "        [ 0.652538  ],\n",
       "        [-0.8679993 ]], dtype=float32),\n",
       " 'b_1': Array([[0.6802069]], dtype=float32),\n",
       " 'w_2': Array([[-1.3727881,  0.1614592]], dtype=float32),\n",
       " 'b_2': Array([[ 0.94661343, -0.31424728]], dtype=float32),\n",
       " 'w_3': Array([[ 0.03668178,  0.23192246, -1.3957725 ,  1.3753045 , -1.1115527 ,\n",
       "         -0.2508165 ],\n",
       "        [-0.44453013, -0.13506305,  1.346293  , -0.08551617, -0.40990335,\n",
       "          0.9052581 ]], dtype=float32),\n",
       " 'b_3': Array([[ 1.5526173 , -1.2139907 , -1.2432982 , -2.2786753 ,  0.6723446 ,\n",
       "         -0.76724064]], dtype=float32),\n",
       " 'w_4': Array([[-0.01430152,  0.3851388 ,  1.215958  ,  1.0629543 , -0.18833871,\n",
       "         -0.36539552, -0.64161634, -0.6946897 ,  0.02955356, -0.45367268],\n",
       "        [ 0.02055193, -0.6459604 , -0.05929086, -1.0947281 , -0.06142029,\n",
       "         -0.80724037,  0.86253947, -0.20581782,  0.39795345,  0.72093195],\n",
       "        [ 0.00943638, -0.02089429, -0.19370487, -0.00423852,  0.7868596 ,\n",
       "          0.82315487,  0.37122995, -0.00596966,  0.1819982 ,  0.04122175],\n",
       "        [-0.17700525,  0.04755846,  0.17755488,  1.0493156 , -0.2303913 ,\n",
       "         -0.1549519 , -0.45284918,  0.15574357, -0.90823835, -0.44882804],\n",
       "        [ 0.06102548,  0.00755686,  0.05081411,  0.67208314, -0.3667469 ,\n",
       "          0.8425206 , -0.20044303, -0.08277798, -0.05542681,  0.7548539 ],\n",
       "        [ 0.56949306,  0.83985394, -0.24707468,  0.1023679 , -0.4463438 ,\n",
       "          0.22817798, -0.3814118 ,  0.8999091 , -0.18570715,  0.09904304]],      dtype=float32),\n",
       " 'b_4': Array([[-1.8925064 , -2.8827593 , -0.4908146 , -0.45247224,  0.6007133 ,\n",
       "          0.32817572,  2.1102817 ,  0.03432975,  0.3172709 ,  0.22939217]],      dtype=float32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neuron_initialization(architecture, seed = 42):\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    key = random.PRNGKey(seed)\n",
    "    for i in range(len(architecture)-1):\n",
    "        \n",
    "        inputs = architecture[i]\n",
    "        oputs = architecture[i+1]\n",
    "        \n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #initializer(subkey, (inputs, oputs), jnp.float32) \n",
    "        \n",
    "        key, subkey = random.split(key)\n",
    "        params[f'w_{i}'] = initializer(subkey, (inputs, oputs), jnp.float32)  #random.uniform(subkey, shape=(inputs, oputs), minval=-1, maxval= 1)     #Weights from neuron to neuron \n",
    "        key, subkey = random.split(key)\n",
    "        params[f'b_{i}'] = initializer(subkey, (1, oputs), jnp.float32)  #random.uniform(subkey, shape=(1, oputs),  minval=-1, maxval= 1)           #Bais vecor for each layer\n",
    "        \n",
    "    return params \n",
    "    \n",
    "architecture = [3,4,1,2,6,10]\n",
    "test_params = neuron_initialization(architecture)\n",
    "test_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.94129837, 0.05870171]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_propagation(params, x_input):\n",
    "    \n",
    "    # could this be removed if we utilize back propagation using jax??\n",
    "    a = x_input\n",
    "    \n",
    "    n_layers = int(len(params)/2)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        w = params[f'w_{i}']\n",
    "        b = params[f'b_{i}']\n",
    "        a_input = a\n",
    "        \n",
    "        z = a_input @ w + b\n",
    "       \n",
    "        if i < n_layers - 1:\n",
    "            a = nn.relu(z)           #general simple activation function is used\n",
    "        else: #problem specific case, for classification we do softmax\n",
    "            a = nn.softmax(z)\n",
    "            \n",
    "        \n",
    "    return a\n",
    "\n",
    "\n",
    "architecture = [2,4,1,2,6,2]\n",
    "test_params = neuron_initialization(architecture)\n",
    "test_input = jnp.array([1,1])\n",
    "\n",
    "a = forward_propagation(test_params, test_input)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4478908\n",
      "loss value 1.4478908\n",
      "grad loss value {'b_0': Array([[0., 0., 0., 0.]], dtype=float32), 'b_1': Array([[0.]], dtype=float32), 'b_2': Array([[0.00221697, 0.        ]], dtype=float32), 'b_3': Array([[0.06043793, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'b_4': Array([[ 0.44129834, -0.4412983 ]], dtype=float32), 'w_0': Array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]], dtype=float32), 'w_1': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32), 'w_2': Array([[0., 0.]], dtype=float32), 'w_3': Array([[0.05721135, 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'w_4': Array([[ 0.7004909 , -0.70049083],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(params, x_input, y_labels, lamba_lasso = 0, lambda_ridge = 0):\n",
    "    y_probs = forward_propagation(params, x_input)\n",
    "    log_probs = jnp.log(y_probs) \n",
    "    one_hot_labels = nn.one_hot(y_labels, y_probs.shape[-1])  # Convert to one-hot encoding, and use the y_probes dims as the num of classes\n",
    "    l = -jnp.mean(jnp.sum(one_hot_labels * log_probs, axis=1))\n",
    "    \n",
    "    if lamba_lasso != 0:\n",
    "        l+= lamba_lasso*jnp.sum(jnp.array([jnp.sum(jnp.abs(params[f'w_{i}'])) for i in range(int(len(params) / 2))]))\n",
    "    if lambda_ridge != 0:\n",
    "        l+= lambda_ridge*jnp.sum(jnp.array([jnp.sum(params[f'w_{i}'] ** 2) for i in range(int(len(params) / 2))]))\n",
    "        \n",
    "    \n",
    "    return l\n",
    "\n",
    "architecture = [2,4,1,2,6,2]\n",
    "test_params = neuron_initialization(architecture)\n",
    "test_input = jnp.array([[1,1], [2,2]])\n",
    "test_labesl = jnp.array([0,1])\n",
    "\n",
    "\n",
    "print(cross_entropy_loss(test_params, test_input, test_labesl))\n",
    "\n",
    "grad(cross_entropy_loss, argnums=0)(test_params, test_input, test_labesl)\n",
    "\n",
    "loss_value, Wb_grad = value_and_grad(cross_entropy_loss, argnums=0)(test_params, test_input, test_labesl)\n",
    "print('loss value', loss_value)\n",
    "print('grad loss value', Wb_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  7]\n",
      " [ 1 10]]\n",
      "[1 1]\n",
      "-----\n",
      "[[78  5]\n",
      " [ 8  1]]\n",
      "[0 0]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def schuffel_data(x, y, seed):\n",
    "    key = jax.random.PRNGKey(seed) \n",
    "    permutation = jax.random.permutation(key, x.shape[0])\n",
    "    return x[permutation], y[permutation]\n",
    "\n",
    "def batch_generator(x_input, y_target, batch_size, schuffel = True, seed = 42):\n",
    "    \n",
    "    n_batches = int(len(y_target)/batch_size)\n",
    "        \n",
    "    #x_batches = jnp.zeros(shape=(n_batches, batch_size, x_input.shape[1]))\n",
    "    #y_batches = jnp.zeros(shape=(n_batches, batch_size))\n",
    "    \n",
    "    if schuffel:     \n",
    "        x_input, y_target = schuffel_data(x_input, y_target, seed)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        #x_batches = x_batches.at[i].set(x_input[i*batch_size:(i+1)*batch_size,:])\n",
    "        #y_batches = y_batches.at[i].set(y_target[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        x_batch = jnp.array(x_input[i*batch_size:(i+1)*batch_size,:])\n",
    "        y_batch = jnp.array(y_target[i*batch_size:(i+1)*batch_size])\n",
    "    \n",
    "        yield x_batch, y_batch\n",
    "    \n",
    "    #return (x_batches, y_batches)\n",
    "\n",
    "x_input = jnp.array([[1, 10], [78, 5], [3, 7], [8, 1]]) \n",
    "y_target = jnp.array([1, 0, 1, 0])\n",
    "batch_size = 2\n",
    "\n",
    "for x_ij, y_ij in batch_generator(x_input, y_target, batch_size):\n",
    "    print(x_ij)\n",
    "    print(y_ij)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_parameter_update(param_grad, params, alpha):\n",
    "   \n",
    "    #updating parameters\n",
    "    updated_params = {}\n",
    "    for param in params.keys():\n",
    "        updated_params[param] = params[param] - alpha*param_grad[param]\n",
    "    \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "\n",
    "    \n",
    "    mean = np.mean(x, axis=0)\n",
    "    std = np.std(x, axis=0)\n",
    "    x_std = (x - mean)/(std + 0.0001)\n",
    "\n",
    "    \n",
    "    return x_std, mean, std\n",
    "\n",
    "def data_split(x, y, split_coeff = 0.8):\n",
    "    n_rows = x.shape[0]\n",
    "    \n",
    "    sc_size = int(split_coeff * n_rows)\n",
    "    \n",
    "    \n",
    "    sc_x = x[:sc_size]\n",
    "    sc_y = y[:sc_size]\n",
    "    \n",
    "    sc_inv_x = x[sc_size:]\n",
    "    sc_inv_y = y[sc_size:]\n",
    "    \n",
    "    return sc_x, sc_y, sc_inv_x, sc_inv_y\n",
    "\n",
    "def prepare_data(x_input, y_target, standerdize = True):\n",
    "\n",
    "\n",
    "    x_train, y_train, x_test, y_test = data_split(x_input, y_target, 0.8)\n",
    "\n",
    "\n",
    "    x_train_n, y_train_n, x_train_v, y_train_v = data_split(x_train, y_train, 0.8)\n",
    "\n",
    "\n",
    "    if standerdize:\n",
    "        \n",
    "        #main traing data\n",
    "        x_std_train, mean_train, std_train = standardize_data(x_train)\n",
    "        x_train  = x_std_train\n",
    "        x_test = (x_test - mean_train)/std_train\n",
    "                \n",
    "        #Validation\n",
    "        x_std_train_n, mean_train_n, std_train_n = standardize_data(x_train_n)\n",
    "        x_train_n = x_std_train_n\n",
    "        x_train_v = (x_train_v - mean_train_n)/std_train_n\n",
    "\n",
    "    return x_train, y_train, x_train_n, y_train_n, x_train_v, y_train_v, x_test, y_test\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x):\n",
    "    y_probs = forward_propagation(params, x)\n",
    "    return np.argmax(y_probs, axis=1)\n",
    "\n",
    "def accuracy(params, x_input, y_target):\n",
    "    y_pred = predict(params, x_input)\n",
    "    return np.mean(y_target == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_data_load(percent = 1):\n",
    "    \n",
    "\n",
    "    \n",
    "    train_df = pd.read_csv('../data/mnist_train.csv')\n",
    "    test_df = pd.read_csv('../data/mnist_test.csv')\n",
    "    \n",
    "    all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    \n",
    "    n_points = len(all_data)\n",
    "    \n",
    "    cut_off_val = int(percent * n_points)\n",
    "    \n",
    "    x_df = all_data.drop('label', axis=1)\n",
    "    y_df = all_data['label']\n",
    "    \n",
    "    x_input = x_df.to_numpy()\n",
    "    y_target = y_df.to_numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return x_input[:cut_off_val], y_target[:cut_off_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train(params, loss, x_input, y_target, batch_size = 25, epochs = 200, alpha = 0.01):\n",
    "    history = {'loss_v_epoch': [], 'accuracy_v_epoch': []}\n",
    "    for i in range(epochs): \n",
    "        j = 0\n",
    "        for x_i_batch, y_i_batch in batch_generator(x_input, y_target, batch_size, schuffel = True, seed = 42): #we go over each batch\n",
    "            loss_i, param_grad = value_and_grad(loss, argnums=0)(params, x_i_batch, y_i_batch)\n",
    "            params = gd_parameter_update(param_grad, params, alpha)\n",
    "           \n",
    "\n",
    "        history['loss_v_epoch'].append(loss_i)\n",
    "\n",
    "        print(f'Epoch {i} -> loss: {loss_i}')\n",
    "\n",
    "            \n",
    "    return params,  history\n",
    "\n",
    "\n",
    "def classification_task(architecture, loss_func, x_input, y_tartget, batch_size = 50, epochs = 10, alpha = 0.01):\n",
    "    #x_input is the numpy array, which then gets convereted to jax in the batching porcces\n",
    "    params = neuron_initialization(architecture)\n",
    "    \n",
    "    #data_proccesing\n",
    "    x_train, y_train, x_train_n, y_train_n, X_train_v, y_train_v, x_test, y_test = prepare_data\n",
    "    \n",
    "    #loss construction\n",
    "    \n",
    "    \n",
    "    \n",
    "    #hyperparameter selection\n",
    "    \n",
    "    \n",
    "    #Training on full set with choosen hyperparameters\n",
    "    \n",
    "    params = classification_train(params, loss, x_train, y_train)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #loggic/plotting info\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input, y_target = mnist_data_load(0.1)\n",
    "x_input = x_input/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> loss: 0.5955199599266052\n",
      "Epoch 1 -> loss: 0.3522457182407379\n",
      "Epoch 2 -> loss: 0.26836323738098145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m data_split(x_input, y_target, split_coeff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#x_train, mean, std = standardize_data(x_train)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(std)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#x_test = (x_test - mean)/(std + 0.0001)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m param_trained, history \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m accuracy(param_trained, x_test, y_test)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mclassification_train\u001b[0;34m(params, loss, x_input, y_target, batch_size, epochs, alpha)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_i_batch, y_i_batch \u001b[38;5;129;01min\u001b[39;00m batch_generator(x_input, y_target, batch_size, schuffel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m): \u001b[38;5;66;03m#we go over each batch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     loss_i, param_grad \u001b[38;5;241m=\u001b[39m value_and_grad(loss, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(params, x_i_batch, y_i_batch)\n\u001b[0;32m----> 7\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mgd_parameter_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_v_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_i)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mgd_parameter_update\u001b[0;34m(param_grad, params, alpha)\u001b[0m\n\u001b[1;32m      4\u001b[0m updated_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 6\u001b[0m     updated_params[param] \u001b[38;5;241m=\u001b[39m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam_grad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_params\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:264\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    262\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = x_input.shape[1]\n",
    "output_dim = 10 \n",
    "architecture = [input_dim,  1000, 1000, output_dim]\n",
    "\n",
    "\n",
    "params = neuron_initialization(architecture)\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = data_split(x_input, y_target, split_coeff = 0.8)\n",
    "#x_train, mean, std = standardize_data(x_train)\n",
    "#print(std)\n",
    "#x_test = (x_test - mean)/(std + 0.0001)\n",
    "\n",
    "\n",
    "param_trained, history = classification_train(params, cross_entropy_loss, x_train, y_train)\n",
    "\n",
    "accuracy(param_trained, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,200,200), history['loss_v_epoch'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240, 128, 128, 128, 128, 128, 128, 10]\n",
      "Epoch 0 -> loss: 1.3829962015151978\n",
      "Epoch 1 -> loss: 0.6392115354537964\n",
      "Epoch 2 -> loss: 0.37645331025123596\n",
      "Epoch 3 -> loss: 0.2513224482536316\n",
      "Epoch 4 -> loss: 0.1911332607269287\n",
      "Epoch 5 -> loss: 0.149545818567276\n",
      "Epoch 6 -> loss: 0.12069046497344971\n",
      "Epoch 7 -> loss: 0.10016915947198868\n",
      "Epoch 8 -> loss: 0.08070912957191467\n",
      "Epoch 9 -> loss: 0.066059909760952\n",
      "Epoch 10 -> loss: 0.05537954717874527\n",
      "Epoch 11 -> loss: 0.0450667068362236\n",
      "Epoch 12 -> loss: 0.039858777076005936\n",
      "Epoch 13 -> loss: 0.03434992581605911\n",
      "Epoch 14 -> loss: 0.029621759429574013\n",
      "Epoch 15 -> loss: 0.02661367505788803\n",
      "Epoch 16 -> loss: 0.0234620850533247\n",
      "Epoch 17 -> loss: 0.021306714043021202\n",
      "Epoch 18 -> loss: 0.018983876332640648\n",
      "Epoch 19 -> loss: 0.017082778736948967\n",
      "Epoch 20 -> loss: 0.015793470665812492\n",
      "Epoch 21 -> loss: 0.014236514456570148\n",
      "Epoch 22 -> loss: 0.013347218744456768\n",
      "Epoch 23 -> loss: 0.012360374443233013\n",
      "Epoch 24 -> loss: 0.01158123929053545\n",
      "Epoch 25 -> loss: 0.010761996731162071\n",
      "Epoch 26 -> loss: 0.010193673893809319\n",
      "Epoch 27 -> loss: 0.009468535892665386\n",
      "Epoch 28 -> loss: 0.008987100794911385\n",
      "Epoch 29 -> loss: 0.008617411367595196\n",
      "Epoch 30 -> loss: 0.008086472749710083\n",
      "Epoch 31 -> loss: 0.007652732543647289\n",
      "Epoch 32 -> loss: 0.007304199039936066\n",
      "Epoch 33 -> loss: 0.006956049241125584\n",
      "Epoch 34 -> loss: 0.006678347010165453\n",
      "Epoch 35 -> loss: 0.0063726333901286125\n",
      "Epoch 36 -> loss: 0.006112526636570692\n",
      "Epoch 37 -> loss: 0.005912090186029673\n",
      "Epoch 38 -> loss: 0.005621070042252541\n",
      "Epoch 39 -> loss: 0.005386407487094402\n",
      "Epoch 40 -> loss: 0.005220599472522736\n",
      "Epoch 41 -> loss: 0.005007513798773289\n",
      "Epoch 42 -> loss: 0.0048691872507333755\n",
      "Epoch 43 -> loss: 0.004672595299780369\n",
      "Epoch 44 -> loss: 0.0045252274721860886\n",
      "Epoch 45 -> loss: 0.004385110456496477\n",
      "Epoch 46 -> loss: 0.004249085206538439\n",
      "Epoch 47 -> loss: 0.004126743413507938\n",
      "Epoch 48 -> loss: 0.003992354031652212\n",
      "Epoch 49 -> loss: 0.003875553607940674\n",
      "Epoch 50 -> loss: 0.0037660091184079647\n",
      "Epoch 51 -> loss: 0.003649740945547819\n",
      "Epoch 52 -> loss: 0.003567514242604375\n",
      "Epoch 53 -> loss: 0.003469603369012475\n",
      "Epoch 54 -> loss: 0.00337314885109663\n",
      "Epoch 55 -> loss: 0.0032795099541544914\n",
      "Epoch 56 -> loss: 0.0031909430399537086\n",
      "Epoch 57 -> loss: 0.003104795701801777\n",
      "Epoch 58 -> loss: 0.003021861892193556\n",
      "Epoch 59 -> loss: 0.002952673938125372\n",
      "Epoch 60 -> loss: 0.0028677359223365784\n",
      "Epoch 61 -> loss: 0.0028052174020558596\n",
      "Epoch 62 -> loss: 0.0027331167366355658\n",
      "Epoch 63 -> loss: 0.0026708580553531647\n",
      "Epoch 64 -> loss: 0.0026006808038800955\n",
      "Epoch 65 -> loss: 0.0025472885463386774\n",
      "Epoch 66 -> loss: 0.00250114593654871\n",
      "Epoch 67 -> loss: 0.0024445001035928726\n",
      "Epoch 68 -> loss: 0.002394893439486623\n",
      "Epoch 69 -> loss: 0.0023487689904868603\n",
      "Epoch 70 -> loss: 0.002295485930517316\n",
      "Epoch 71 -> loss: 0.002255875850096345\n",
      "Epoch 72 -> loss: 0.0022135621402412653\n",
      "Epoch 73 -> loss: 0.0021791718900203705\n",
      "Epoch 74 -> loss: 0.0021280357614159584\n",
      "Epoch 75 -> loss: 0.0020913751795887947\n",
      "Epoch 76 -> loss: 0.0020613919477909803\n",
      "Epoch 77 -> loss: 0.002022451488301158\n",
      "Epoch 78 -> loss: 0.0019886004738509655\n",
      "Epoch 79 -> loss: 0.001959584653377533\n",
      "Epoch 80 -> loss: 0.0019234255887567997\n",
      "Epoch 81 -> loss: 0.0018894135719165206\n",
      "Epoch 82 -> loss: 0.0018535458948463202\n",
      "Epoch 83 -> loss: 0.0018279687501490116\n",
      "Epoch 84 -> loss: 0.0018009503837674856\n",
      "Epoch 85 -> loss: 0.0017722576158121228\n",
      "Epoch 86 -> loss: 0.0017480176175013185\n",
      "Epoch 87 -> loss: 0.0017169065540656447\n",
      "Epoch 88 -> loss: 0.001691437908448279\n",
      "Epoch 89 -> loss: 0.0016649155877530575\n",
      "Epoch 90 -> loss: 0.001644815201871097\n",
      "Epoch 91 -> loss: 0.0016190004535019398\n",
      "Epoch 92 -> loss: 0.0015958747826516628\n",
      "Epoch 93 -> loss: 0.0015764678828418255\n",
      "Epoch 94 -> loss: 0.0015514809638261795\n",
      "Epoch 95 -> loss: 0.0015330263413488865\n",
      "Epoch 96 -> loss: 0.0015105684287846088\n",
      "Epoch 97 -> loss: 0.0014907853910699487\n",
      "Epoch 98 -> loss: 0.001471986761316657\n",
      "Epoch 99 -> loss: 0.001451220829039812\n",
      "Epoch 100 -> loss: 0.0014312932034954429\n",
      "Epoch 101 -> loss: 0.001414027763530612\n",
      "Epoch 102 -> loss: 0.001397969201207161\n",
      "Epoch 103 -> loss: 0.0013783863978460431\n",
      "Epoch 104 -> loss: 0.001361967297270894\n",
      "Epoch 105 -> loss: 0.0013429835671558976\n",
      "Epoch 106 -> loss: 0.0013297977857291698\n",
      "Epoch 107 -> loss: 0.001313884393312037\n",
      "Epoch 108 -> loss: 0.0012962957844138145\n",
      "Epoch 109 -> loss: 0.0012798034586012363\n",
      "Epoch 110 -> loss: 0.001266569015569985\n",
      "Epoch 111 -> loss: 0.0012507066130638123\n",
      "Epoch 112 -> loss: 0.0012381008127704263\n",
      "Epoch 113 -> loss: 0.001222661230713129\n",
      "Epoch 114 -> loss: 0.0012096434365957975\n",
      "Epoch 115 -> loss: 0.0011949919862672687\n",
      "Epoch 116 -> loss: 0.0011822511442005634\n",
      "Epoch 117 -> loss: 0.0011700689792633057\n",
      "Epoch 118 -> loss: 0.0011578781995922327\n",
      "Epoch 119 -> loss: 0.00114692491479218\n",
      "Epoch 120 -> loss: 0.0011356885079294443\n",
      "Epoch 121 -> loss: 0.0011220256565138698\n",
      "Epoch 122 -> loss: 0.0011102503631263971\n",
      "Epoch 123 -> loss: 0.001098726992495358\n",
      "Epoch 124 -> loss: 0.0010880748741328716\n",
      "Epoch 125 -> loss: 0.0010764143662527204\n",
      "Epoch 126 -> loss: 0.0010666601592674851\n",
      "Epoch 127 -> loss: 0.0010549024445936084\n",
      "Epoch 128 -> loss: 0.001045095850713551\n",
      "Epoch 129 -> loss: 0.0010330287041142583\n",
      "Epoch 130 -> loss: 0.0010243220021948218\n",
      "Epoch 131 -> loss: 0.0010120609076693654\n",
      "Epoch 132 -> loss: 0.0010043020593002439\n",
      "Epoch 133 -> loss: 0.0009923906764015555\n",
      "Epoch 134 -> loss: 0.0009854803793132305\n",
      "Epoch 135 -> loss: 0.0009730163146741688\n",
      "Epoch 136 -> loss: 0.0009644737583585083\n",
      "Epoch 137 -> loss: 0.0009545336361043155\n",
      "Epoch 138 -> loss: 0.0009459508582949638\n",
      "Epoch 139 -> loss: 0.0009378946851938963\n",
      "Epoch 140 -> loss: 0.000928290537558496\n",
      "Epoch 141 -> loss: 0.000921337865293026\n",
      "Epoch 142 -> loss: 0.0009111035615205765\n",
      "Epoch 143 -> loss: 0.0009045122424140573\n",
      "Epoch 144 -> loss: 0.0008959640399552882\n",
      "Epoch 145 -> loss: 0.0008874906343407929\n",
      "Epoch 146 -> loss: 0.0008799366187304258\n",
      "Epoch 147 -> loss: 0.000871638476382941\n",
      "Epoch 148 -> loss: 0.000865443900693208\n",
      "Epoch 149 -> loss: 0.0008575443644076586\n",
      "Epoch 150 -> loss: 0.0008502573473379016\n",
      "Epoch 151 -> loss: 0.0008422419778071344\n",
      "Epoch 152 -> loss: 0.0008355659083463252\n",
      "Epoch 153 -> loss: 0.0008280262118205428\n",
      "Epoch 154 -> loss: 0.0008211920503526926\n",
      "Epoch 155 -> loss: 0.0008135541575029492\n",
      "Epoch 156 -> loss: 0.0008079829858615994\n",
      "Epoch 157 -> loss: 0.0008009816519916058\n",
      "Epoch 158 -> loss: 0.0007944881799630821\n",
      "Epoch 159 -> loss: 0.0007883537909947336\n",
      "Epoch 160 -> loss: 0.0007814110722392797\n",
      "Epoch 161 -> loss: 0.0007756544509902596\n",
      "Epoch 162 -> loss: 0.0007698086556047201\n",
      "Epoch 163 -> loss: 0.0007635081419721246\n",
      "Epoch 164 -> loss: 0.0007580320234410465\n",
      "Epoch 165 -> loss: 0.0007528749410994351\n",
      "Epoch 166 -> loss: 0.0007466679671779275\n",
      "Epoch 167 -> loss: 0.0007410667021758854\n",
      "Epoch 168 -> loss: 0.0007357589202001691\n",
      "Epoch 169 -> loss: 0.0007299232529476285\n",
      "Epoch 170 -> loss: 0.0007249501650221646\n",
      "Epoch 171 -> loss: 0.0007201379630714655\n",
      "Epoch 172 -> loss: 0.0007151770405471325\n",
      "Epoch 173 -> loss: 0.0007092556916177273\n",
      "Epoch 174 -> loss: 0.0007049141568131745\n",
      "Epoch 175 -> loss: 0.000699276220984757\n",
      "Epoch 176 -> loss: 0.0006955639692023396\n",
      "Epoch 177 -> loss: 0.0006895411061123013\n",
      "Epoch 178 -> loss: 0.0006851358339190483\n",
      "Epoch 179 -> loss: 0.0006799040711484849\n",
      "Epoch 180 -> loss: 0.000674894661642611\n",
      "Epoch 181 -> loss: 0.0006709194276481867\n",
      "Epoch 182 -> loss: 0.0006661902880296111\n",
      "Epoch 183 -> loss: 0.0006617230246774852\n",
      "Epoch 184 -> loss: 0.0006562600610777736\n",
      "Epoch 185 -> loss: 0.0006522737094201148\n",
      "Epoch 186 -> loss: 0.0006482055177912116\n",
      "Epoch 187 -> loss: 0.0006441636942327023\n",
      "Epoch 188 -> loss: 0.0006389768095687032\n",
      "Epoch 189 -> loss: 0.0006350242765620351\n",
      "Epoch 190 -> loss: 0.0006307786679826677\n",
      "Epoch 191 -> loss: 0.0006268825964070857\n",
      "Epoch 192 -> loss: 0.0006225040415301919\n",
      "Epoch 193 -> loss: 0.0006181559874676168\n",
      "Epoch 194 -> loss: 0.0006143629434518516\n",
      "Epoch 195 -> loss: 0.0006106008659116924\n",
      "Epoch 196 -> loss: 0.0006064089830033481\n",
      "Epoch 197 -> loss: 0.0006033489480614662\n",
      "Epoch 198 -> loss: 0.0005989882629364729\n",
      "Epoch 199 -> loss: 0.000595788995269686\n",
      "Epoch 200 -> loss: 0.0005915488582104445\n",
      "Epoch 201 -> loss: 0.0005879784002900124\n",
      "Epoch 202 -> loss: 0.0005848237196914852\n",
      "Epoch 203 -> loss: 0.0005813814350403845\n",
      "Epoch 204 -> loss: 0.00057745206868276\n",
      "Epoch 205 -> loss: 0.0005736788152717054\n",
      "Epoch 206 -> loss: 0.0005704172072000802\n",
      "Epoch 207 -> loss: 0.0005669078091159463\n",
      "Epoch 208 -> loss: 0.0005634073168039322\n",
      "Epoch 209 -> loss: 0.000560556713026017\n",
      "Epoch 210 -> loss: 0.0005572870722971857\n",
      "Epoch 211 -> loss: 0.0005539748817682266\n",
      "Epoch 212 -> loss: 0.0005505523295141757\n",
      "Epoch 213 -> loss: 0.0005468040471896529\n",
      "Epoch 214 -> loss: 0.0005445912829600275\n",
      "Epoch 215 -> loss: 0.0005408419528976083\n",
      "Epoch 216 -> loss: 0.0005383410025388002\n",
      "Epoch 217 -> loss: 0.0005349944694899023\n",
      "Epoch 218 -> loss: 0.000531675003003329\n",
      "Epoch 219 -> loss: 0.000529430340975523\n",
      "Epoch 220 -> loss: 0.0005254788557067513\n",
      "Epoch 221 -> loss: 0.0005227658548392355\n",
      "Epoch 222 -> loss: 0.000519918801728636\n",
      "Epoch 223 -> loss: 0.0005173462559469044\n",
      "Epoch 224 -> loss: 0.0005141675937920809\n",
      "Epoch 225 -> loss: 0.0005115306121297181\n",
      "Epoch 226 -> loss: 0.0005086589371785522\n",
      "Epoch 227 -> loss: 0.0005059196846559644\n",
      "Epoch 228 -> loss: 0.0005031166365370154\n",
      "Epoch 229 -> loss: 0.0005004368140362203\n",
      "Epoch 230 -> loss: 0.0004973695031367242\n",
      "Epoch 231 -> loss: 0.0004948677378706634\n",
      "Epoch 232 -> loss: 0.0004924852983094752\n",
      "Epoch 233 -> loss: 0.0004896328318864107\n",
      "Epoch 234 -> loss: 0.0004875301383435726\n",
      "Epoch 235 -> loss: 0.000484501535538584\n",
      "Epoch 236 -> loss: 0.0004825317009817809\n",
      "Epoch 237 -> loss: 0.00048005947610363364\n",
      "Epoch 238 -> loss: 0.0004774715634994209\n",
      "Epoch 239 -> loss: 0.00047521537635475397\n",
      "Epoch 240 -> loss: 0.00047247152542695403\n",
      "Epoch 241 -> loss: 0.00047039080527611077\n",
      "Epoch 242 -> loss: 0.00046774811926297843\n",
      "Epoch 243 -> loss: 0.00046518578892573714\n",
      "Epoch 244 -> loss: 0.0004631933115888387\n",
      "Epoch 245 -> loss: 0.00046072242548689246\n",
      "Epoch 246 -> loss: 0.0004583967092912644\n",
      "Epoch 247 -> loss: 0.0004559177905321121\n",
      "Epoch 248 -> loss: 0.0004538287175819278\n",
      "Epoch 249 -> loss: 0.00045167922507971525\n",
      "Epoch 250 -> loss: 0.0004491655563469976\n",
      "Epoch 251 -> loss: 0.00044756318675354123\n",
      "Epoch 252 -> loss: 0.00044507940765470266\n",
      "Epoch 253 -> loss: 0.0004430013068486005\n",
      "Epoch 254 -> loss: 0.0004409976245369762\n",
      "Epoch 255 -> loss: 0.0004390145477373153\n",
      "Epoch 256 -> loss: 0.000436833273852244\n",
      "Epoch 257 -> loss: 0.00043482138426043093\n",
      "Epoch 258 -> loss: 0.00043245923006907105\n",
      "Epoch 259 -> loss: 0.00043099900358356535\n",
      "Epoch 260 -> loss: 0.0004290135984774679\n",
      "Epoch 261 -> loss: 0.00042658543679863214\n",
      "Epoch 262 -> loss: 0.0004246991884429008\n",
      "Epoch 263 -> loss: 0.0004230709746479988\n",
      "Epoch 264 -> loss: 0.0004210997140035033\n",
      "Epoch 265 -> loss: 0.00041943287942558527\n",
      "Epoch 266 -> loss: 0.000417401926824823\n",
      "Epoch 267 -> loss: 0.0004153400659561157\n",
      "Epoch 268 -> loss: 0.0004134389746468514\n",
      "Epoch 269 -> loss: 0.00041191428317688406\n",
      "Epoch 270 -> loss: 0.00040978952893055975\n",
      "Epoch 271 -> loss: 0.0004081663500983268\n",
      "Epoch 272 -> loss: 0.00040637809433974326\n",
      "Epoch 273 -> loss: 0.00040465741767548025\n",
      "Epoch 274 -> loss: 0.00040288057061843574\n",
      "Epoch 275 -> loss: 0.00040095343138091266\n",
      "Epoch 276 -> loss: 0.000399179378291592\n",
      "Epoch 277 -> loss: 0.0003975713043473661\n",
      "Epoch 278 -> loss: 0.0003960891626775265\n",
      "Epoch 279 -> loss: 0.00039401493268087506\n",
      "Epoch 280 -> loss: 0.0003925615455955267\n",
      "Epoch 281 -> loss: 0.00039080908754840493\n",
      "Epoch 282 -> loss: 0.0003891907981596887\n",
      "Epoch 283 -> loss: 0.0003878475690726191\n",
      "Epoch 284 -> loss: 0.0003860784345306456\n",
      "Epoch 285 -> loss: 0.000384338665753603\n",
      "Epoch 286 -> loss: 0.0003828721819445491\n",
      "Epoch 287 -> loss: 0.0003812437935266644\n",
      "Epoch 288 -> loss: 0.0003796952951233834\n",
      "Epoch 289 -> loss: 0.00037820436409674585\n",
      "Epoch 290 -> loss: 0.0003765439032576978\n",
      "Epoch 291 -> loss: 0.00037517613964155316\n",
      "Epoch 292 -> loss: 0.0003734376805368811\n",
      "Epoch 293 -> loss: 0.00037206007982604206\n",
      "Epoch 294 -> loss: 0.00037046088255010545\n",
      "Epoch 295 -> loss: 0.00036894797813147306\n",
      "Epoch 296 -> loss: 0.00036744854878634214\n",
      "Epoch 297 -> loss: 0.00036603910848498344\n",
      "Epoch 298 -> loss: 0.00036455190274864435\n",
      "Epoch 299 -> loss: 0.000363255589036271\n",
      "Epoch 300 -> loss: 0.0003616479807533324\n",
      "Epoch 301 -> loss: 0.00036039832048118114\n",
      "Epoch 302 -> loss: 0.00035891347215510905\n",
      "Epoch 303 -> loss: 0.00035753464908339083\n",
      "Epoch 304 -> loss: 0.00035595669760368764\n",
      "Epoch 305 -> loss: 0.00035479330108501017\n",
      "Epoch 306 -> loss: 0.0003534085117280483\n",
      "Epoch 307 -> loss: 0.00035220803692936897\n",
      "Epoch 308 -> loss: 0.0003506348584778607\n",
      "Epoch 309 -> loss: 0.000349499867297709\n",
      "Epoch 310 -> loss: 0.00034787796903401613\n",
      "Epoch 311 -> loss: 0.0003468116628937423\n",
      "Epoch 312 -> loss: 0.0003455402620602399\n",
      "Epoch 313 -> loss: 0.0003440020082052797\n",
      "Epoch 314 -> loss: 0.00034273616620339453\n",
      "Epoch 315 -> loss: 0.0003416564140934497\n",
      "Epoch 316 -> loss: 0.00034019036684185266\n",
      "Epoch 317 -> loss: 0.00033899012487381697\n",
      "Epoch 318 -> loss: 0.0003378279507160187\n",
      "Epoch 319 -> loss: 0.0003366064338479191\n",
      "Epoch 320 -> loss: 0.00033533258829265833\n",
      "Epoch 321 -> loss: 0.0003341160772833973\n",
      "Epoch 322 -> loss: 0.0003329065511934459\n",
      "Epoch 323 -> loss: 0.00033163875923492014\n",
      "Epoch 324 -> loss: 0.00033039908157661557\n",
      "Epoch 325 -> loss: 0.00032926484709605575\n",
      "Epoch 326 -> loss: 0.0003282733086962253\n",
      "Epoch 327 -> loss: 0.0003267589781899005\n",
      "Epoch 328 -> loss: 0.00032569741597399116\n",
      "Epoch 329 -> loss: 0.00032457298948429525\n",
      "Epoch 330 -> loss: 0.00032350674155168235\n",
      "Epoch 331 -> loss: 0.00032216942054219544\n",
      "Epoch 332 -> loss: 0.0003211823059245944\n",
      "Epoch 333 -> loss: 0.0003199501952622086\n",
      "Epoch 334 -> loss: 0.0003189592098351568\n",
      "Epoch 335 -> loss: 0.0003178347833454609\n",
      "Epoch 336 -> loss: 0.0003168318362440914\n",
      "Epoch 337 -> loss: 0.0003156774037051946\n",
      "Epoch 338 -> loss: 0.00031486459192819893\n",
      "Epoch 339 -> loss: 0.00031348795164376497\n",
      "Epoch 340 -> loss: 0.00031257234513759613\n",
      "Epoch 341 -> loss: 0.00031150112044997513\n",
      "Epoch 342 -> loss: 0.00031041979673318565\n",
      "Epoch 343 -> loss: 0.0003094192943535745\n",
      "Epoch 344 -> loss: 0.00030834603239782155\n",
      "Epoch 345 -> loss: 0.0003073646512348205\n",
      "Epoch 346 -> loss: 0.00030636321753263474\n",
      "Epoch 347 -> loss: 0.00030528634670190513\n",
      "Epoch 348 -> loss: 0.000304278772091493\n",
      "Epoch 349 -> loss: 0.0003033837128896266\n",
      "Epoch 350 -> loss: 0.00030233649886213243\n",
      "Epoch 351 -> loss: 0.00030115919071249664\n",
      "Epoch 352 -> loss: 0.00030033235088922083\n",
      "Epoch 353 -> loss: 0.00029944293783046305\n",
      "Epoch 354 -> loss: 0.0002984258462674916\n",
      "Epoch 355 -> loss: 0.0002975617244374007\n",
      "Epoch 356 -> loss: 0.0002964153536595404\n",
      "Epoch 357 -> loss: 0.00029545326833613217\n",
      "Epoch 358 -> loss: 0.00029463082319125533\n",
      "Epoch 359 -> loss: 0.0002936996752396226\n",
      "Epoch 360 -> loss: 0.00029265027842484415\n",
      "Epoch 361 -> loss: 0.00029191141948103905\n",
      "Epoch 362 -> loss: 0.000290938769467175\n",
      "Epoch 363 -> loss: 0.00029010180151090026\n",
      "Epoch 364 -> loss: 0.00028913604910485446\n",
      "Epoch 365 -> loss: 0.0002883029228542\n",
      "Epoch 366 -> loss: 0.0002875665668398142\n",
      "Epoch 367 -> loss: 0.00028662855038419366\n",
      "Epoch 368 -> loss: 0.00028570674476213753\n",
      "Epoch 369 -> loss: 0.0002848259173333645\n",
      "Epoch 370 -> loss: 0.00028395920526236296\n",
      "Epoch 371 -> loss: 0.00028313821530900896\n",
      "Epoch 372 -> loss: 0.00028229906456544995\n",
      "Epoch 373 -> loss: 0.00028149198624305427\n",
      "Epoch 374 -> loss: 0.0002806567645166069\n",
      "Epoch 375 -> loss: 0.00027974200202152133\n",
      "Epoch 376 -> loss: 0.0002789379213936627\n",
      "Epoch 377 -> loss: 0.00027814172790385783\n",
      "Epoch 378 -> loss: 0.0002772892767097801\n",
      "Epoch 379 -> loss: 0.00027655428857542574\n",
      "Epoch 380 -> loss: 0.0002757651964202523\n",
      "Epoch 381 -> loss: 0.0002749034611042589\n",
      "Epoch 382 -> loss: 0.00027406305889599025\n",
      "Epoch 383 -> loss: 0.00027328377473168075\n",
      "Epoch 384 -> loss: 0.0002724422374740243\n",
      "Epoch 385 -> loss: 0.00027162313926965\n",
      "Epoch 386 -> loss: 0.00027083687018603086\n",
      "Epoch 387 -> loss: 0.0002701754856389016\n",
      "Epoch 388 -> loss: 0.00026929116575047374\n",
      "Epoch 389 -> loss: 0.0002685117069631815\n",
      "Epoch 390 -> loss: 0.0002677620213944465\n",
      "Epoch 391 -> loss: 0.0002670771209523082\n",
      "Epoch 392 -> loss: 0.00026623907615430653\n",
      "Epoch 393 -> loss: 0.0002654286508914083\n",
      "Epoch 394 -> loss: 0.00026483312831260264\n",
      "Epoch 395 -> loss: 0.00026406472898088396\n",
      "Epoch 396 -> loss: 0.00026337106828577816\n",
      "Epoch 397 -> loss: 0.00026259172591380775\n",
      "Epoch 398 -> loss: 0.0002617513819132\n",
      "Epoch 399 -> loss: 0.000261047127423808\n",
      "Epoch 400 -> loss: 0.000260365690337494\n",
      "Epoch 401 -> loss: 0.00025966271641664207\n",
      "Epoch 402 -> loss: 0.00025901952176354825\n",
      "Epoch 403 -> loss: 0.0002582544111646712\n",
      "Epoch 404 -> loss: 0.00025757274124771357\n",
      "Epoch 405 -> loss: 0.00025687352172099054\n",
      "Epoch 406 -> loss: 0.00025615139747969806\n",
      "Epoch 407 -> loss: 0.0002555224928073585\n",
      "Epoch 408 -> loss: 0.00025479792384430766\n",
      "Epoch 409 -> loss: 0.00025414052652195096\n",
      "Epoch 410 -> loss: 0.0002533346996642649\n",
      "Epoch 411 -> loss: 0.00025263160932809114\n",
      "Epoch 412 -> loss: 0.0002520577108953148\n",
      "Epoch 413 -> loss: 0.0002513188519515097\n",
      "Epoch 414 -> loss: 0.00025063156499527395\n",
      "Epoch 415 -> loss: 0.0002499762049410492\n",
      "Epoch 416 -> loss: 0.0002493030624464154\n",
      "Epoch 417 -> loss: 0.0002486109733581543\n",
      "Epoch 418 -> loss: 0.0002478875103406608\n",
      "Epoch 419 -> loss: 0.0002472788328304887\n",
      "Epoch 420 -> loss: 0.0002465650613885373\n",
      "Epoch 421 -> loss: 0.00024585507344454527\n",
      "Epoch 422 -> loss: 0.00024534069234505296\n",
      "Epoch 423 -> loss: 0.00024457689141854644\n",
      "Epoch 424 -> loss: 0.00024402777489740402\n",
      "Epoch 425 -> loss: 0.0002433355839457363\n",
      "Epoch 426 -> loss: 0.00024266609398182482\n",
      "Epoch 427 -> loss: 0.00024205386580433697\n",
      "Epoch 428 -> loss: 0.00024136630236171186\n",
      "Epoch 429 -> loss: 0.0002408317814115435\n",
      "Epoch 430 -> loss: 0.00024012535868678242\n",
      "Epoch 431 -> loss: 0.00023953798518050462\n",
      "Epoch 432 -> loss: 0.00023890200827736408\n",
      "Epoch 433 -> loss: 0.00023830757709220052\n",
      "Epoch 434 -> loss: 0.0002376799238845706\n",
      "Epoch 435 -> loss: 0.00023710477398708463\n",
      "Epoch 436 -> loss: 0.00023638618586119264\n",
      "Epoch 437 -> loss: 0.00023585154849570245\n",
      "Epoch 438 -> loss: 0.00023519281239714473\n",
      "Epoch 439 -> loss: 0.0002346341934753582\n",
      "Epoch 440 -> loss: 0.00023406039690598845\n",
      "Epoch 441 -> loss: 0.00023347426031250507\n",
      "Epoch 442 -> loss: 0.00023288483498618007\n",
      "Epoch 443 -> loss: 0.00023220568255055696\n",
      "Epoch 444 -> loss: 0.00023167222389020026\n",
      "Epoch 445 -> loss: 0.00023103252169676125\n",
      "Epoch 446 -> loss: 0.00023064968991093338\n",
      "Epoch 447 -> loss: 0.0002299131010659039\n",
      "Epoch 448 -> loss: 0.00022933924628887326\n",
      "Epoch 449 -> loss: 0.00022883806377649307\n",
      "Epoch 450 -> loss: 0.0002282041823491454\n",
      "Epoch 451 -> loss: 0.00022764464665669948\n",
      "Epoch 452 -> loss: 0.00022707776224706322\n",
      "Epoch 453 -> loss: 0.00022652048210147768\n",
      "Epoch 454 -> loss: 0.00022596308554057032\n",
      "Epoch 455 -> loss: 0.00022537130280397832\n",
      "Epoch 456 -> loss: 0.00022479954350274056\n",
      "Epoch 457 -> loss: 0.0002243510534754023\n",
      "Epoch 458 -> loss: 0.0002237399312434718\n",
      "Epoch 459 -> loss: 0.000223215960431844\n",
      "Epoch 460 -> loss: 0.00022269939654506743\n",
      "Epoch 461 -> loss: 0.00022207274741958827\n",
      "Epoch 462 -> loss: 0.0002216146094724536\n",
      "Epoch 463 -> loss: 0.0002210488892160356\n",
      "Epoch 464 -> loss: 0.00022043428907636553\n",
      "Epoch 465 -> loss: 0.00021994033886585385\n",
      "Epoch 466 -> loss: 0.00021937456040177494\n",
      "Epoch 467 -> loss: 0.00021888644550926983\n",
      "Epoch 468 -> loss: 0.00021836395899299532\n",
      "Epoch 469 -> loss: 0.00021787699370179325\n",
      "Epoch 470 -> loss: 0.0002172659442294389\n",
      "Epoch 471 -> loss: 0.00021678031771443784\n",
      "Epoch 472 -> loss: 0.00021625759836751968\n",
      "Epoch 473 -> loss: 0.0002158042771043256\n",
      "Epoch 474 -> loss: 0.00021524089970625937\n",
      "Epoch 475 -> loss: 0.00021470864885486662\n",
      "Epoch 476 -> loss: 0.00021423146245069802\n",
      "Epoch 477 -> loss: 0.00021369667956605554\n",
      "Epoch 478 -> loss: 0.00021320281666703522\n",
      "Epoch 479 -> loss: 0.0002127075131284073\n",
      "Epoch 480 -> loss: 0.00021217527682892978\n",
      "Epoch 481 -> loss: 0.00021173387358430773\n",
      "Epoch 482 -> loss: 0.00021117768483236432\n",
      "Epoch 483 -> loss: 0.00021064316388219595\n",
      "Epoch 484 -> loss: 0.0002101657446473837\n",
      "Epoch 485 -> loss: 0.00020967421005479991\n",
      "Epoch 486 -> loss: 0.00020916351058986038\n",
      "Epoch 487 -> loss: 0.00020868133287876844\n",
      "Epoch 488 -> loss: 0.0002081814018310979\n",
      "Epoch 489 -> loss: 0.00020771841809619218\n",
      "Epoch 490 -> loss: 0.000207181423320435\n",
      "Epoch 491 -> loss: 0.00020669940568041056\n",
      "Epoch 492 -> loss: 0.00020623274031095207\n",
      "Epoch 493 -> loss: 0.00020573522488120943\n",
      "Epoch 494 -> loss: 0.0002053175849141553\n",
      "Epoch 495 -> loss: 0.00020480930106714368\n",
      "Epoch 496 -> loss: 0.00020433666941244155\n",
      "Epoch 497 -> loss: 0.00020385111565701663\n",
      "Epoch 498 -> loss: 0.00020335336739663035\n",
      "Epoch 499 -> loss: 0.00020292394037824124\n",
      "Epoch 500 -> loss: 0.0002024358109338209\n",
      "Epoch 501 -> loss: 0.00020202658197376877\n",
      "Epoch 502 -> loss: 0.00020148366456851363\n",
      "Epoch 503 -> loss: 0.00020104933355469257\n",
      "Epoch 504 -> loss: 0.00020059816597495228\n",
      "Epoch 505 -> loss: 0.0002001663960982114\n",
      "Epoch 506 -> loss: 0.00019961253565270454\n",
      "Epoch 507 -> loss: 0.00019923441868741065\n",
      "Epoch 508 -> loss: 0.00019875231373589486\n",
      "Epoch 509 -> loss: 0.00019832748512271792\n",
      "Epoch 510 -> loss: 0.00019786329357884824\n",
      "Epoch 511 -> loss: 0.00019738837727345526\n",
      "Epoch 512 -> loss: 0.00019696596427820623\n",
      "Epoch 513 -> loss: 0.00019650068134069443\n",
      "Epoch 514 -> loss: 0.00019606509886216372\n",
      "Epoch 515 -> loss: 0.0001955806656042114\n",
      "Epoch 516 -> loss: 0.00019517973123583943\n",
      "Epoch 517 -> loss: 0.0001946844713529572\n",
      "Epoch 518 -> loss: 0.00019430748943705112\n",
      "Epoch 519 -> loss: 0.00019383012840989977\n",
      "Epoch 520 -> loss: 0.00019339937716722488\n",
      "Epoch 521 -> loss: 0.00019289582269266248\n",
      "Epoch 522 -> loss: 0.00019253551727160811\n",
      "Epoch 523 -> loss: 0.00019206886645406485\n",
      "Epoch 524 -> loss: 0.00019166087440680712\n",
      "Epoch 525 -> loss: 0.00019121452351100743\n",
      "Epoch 526 -> loss: 0.00019078499462921172\n",
      "Epoch 527 -> loss: 0.00019036975572817028\n",
      "Epoch 528 -> loss: 0.0001899258786579594\n",
      "Epoch 529 -> loss: 0.00018947958596982062\n",
      "Epoch 530 -> loss: 0.0001890834973892197\n",
      "Epoch 531 -> loss: 0.0001887207035906613\n",
      "Epoch 532 -> loss: 0.00018821719277184457\n",
      "Epoch 533 -> loss: 0.0001878126640804112\n",
      "Epoch 534 -> loss: 0.00018745477427728474\n",
      "Epoch 535 -> loss: 0.00018698934582062066\n",
      "Epoch 536 -> loss: 0.00018665060633793473\n",
      "Epoch 537 -> loss: 0.00018618635658640414\n",
      "Epoch 538 -> loss: 0.00018583079508971423\n",
      "Epoch 539 -> loss: 0.00018542632460594177\n",
      "Epoch 540 -> loss: 0.00018506127526052296\n",
      "Epoch 541 -> loss: 0.0001846065861172974\n",
      "Epoch 542 -> loss: 0.00018422484572511166\n",
      "Epoch 543 -> loss: 0.00018381666450295597\n",
      "Epoch 544 -> loss: 0.0001834134745877236\n",
      "Epoch 545 -> loss: 0.00018302923126611859\n",
      "Epoch 546 -> loss: 0.00018267362611368299\n",
      "Epoch 547 -> loss: 0.00018227634427603334\n",
      "Epoch 548 -> loss: 0.00018191961862612516\n",
      "Epoch 549 -> loss: 0.00018150079995393753\n",
      "Epoch 550 -> loss: 0.00018112134421244264\n",
      "Epoch 551 -> loss: 0.00018083509348798543\n",
      "Epoch 552 -> loss: 0.00018042101874016225\n",
      "Epoch 553 -> loss: 0.000180047529283911\n",
      "Epoch 554 -> loss: 0.00017966098675969988\n",
      "Epoch 555 -> loss: 0.00017929110617842525\n",
      "Epoch 556 -> loss: 0.00017892006144393235\n",
      "Epoch 557 -> loss: 0.00017860266962088645\n",
      "Epoch 558 -> loss: 0.00017817191837821156\n",
      "Epoch 559 -> loss: 0.00017782360373530537\n",
      "Epoch 560 -> loss: 0.000177506182808429\n",
      "Epoch 561 -> loss: 0.00017712324915919453\n",
      "Epoch 562 -> loss: 0.0001767484936863184\n",
      "Epoch 563 -> loss: 0.00017638946883380413\n",
      "Epoch 564 -> loss: 0.00017603383457753807\n",
      "Epoch 565 -> loss: 0.00017566159658599645\n",
      "Epoch 566 -> loss: 0.00017534424841869622\n",
      "Epoch 567 -> loss: 0.00017496604414191097\n",
      "Epoch 568 -> loss: 0.0001745889603625983\n",
      "Epoch 569 -> loss: 0.0001742381718941033\n",
      "Epoch 570 -> loss: 0.00017389582353644073\n",
      "Epoch 571 -> loss: 0.00017352464783471078\n",
      "Epoch 572 -> loss: 0.00017316918820142746\n",
      "Epoch 573 -> loss: 0.00017286492220591754\n",
      "Epoch 574 -> loss: 0.00017249505617655814\n",
      "Epoch 575 -> loss: 0.00017215259140357375\n",
      "Epoch 576 -> loss: 0.00017185203614644706\n",
      "Epoch 577 -> loss: 0.00017141162243206054\n",
      "Epoch 578 -> loss: 0.00017114925140049309\n",
      "Epoch 579 -> loss: 0.0001707697956589982\n",
      "Epoch 580 -> loss: 0.00017041548562701792\n",
      "Epoch 581 -> loss: 0.00017007662972901016\n",
      "Epoch 582 -> loss: 0.00016979743668343872\n",
      "Epoch 583 -> loss: 0.00016938823682721704\n",
      "Epoch 584 -> loss: 0.00016909830446820706\n",
      "Epoch 585 -> loss: 0.00016874517314136028\n",
      "Epoch 586 -> loss: 0.00016840510943438858\n",
      "Epoch 587 -> loss: 0.00016808063082862645\n",
      "Epoch 588 -> loss: 0.00016776687698438764\n",
      "Epoch 589 -> loss: 0.00016740649880375713\n",
      "Epoch 590 -> loss: 0.00016712021897546947\n",
      "Epoch 591 -> loss: 0.0001667551405262202\n",
      "Epoch 592 -> loss: 0.00016647113079670817\n",
      "Epoch 593 -> loss: 0.0001661275455262512\n",
      "Epoch 594 -> loss: 0.00016583531396463513\n",
      "Epoch 595 -> loss: 0.0001654737425269559\n",
      "Epoch 596 -> loss: 0.00016514211893081665\n",
      "Epoch 597 -> loss: 0.00016484622028656304\n",
      "Epoch 598 -> loss: 0.00016456229786854237\n",
      "Epoch 599 -> loss: 0.00016422350017819554\n",
      "Epoch 600 -> loss: 0.00016393954865634441\n",
      "Epoch 601 -> loss: 0.00016358877473976463\n",
      "Epoch 602 -> loss: 0.00016334187239408493\n",
      "Epoch 603 -> loss: 0.00016296602552756667\n",
      "Epoch 604 -> loss: 0.00016270834021270275\n",
      "Epoch 605 -> loss: 0.00016235995281022042\n",
      "Epoch 606 -> loss: 0.0001621046831132844\n",
      "Epoch 607 -> loss: 0.0001617622037883848\n",
      "Epoch 608 -> loss: 0.00016151177987921983\n",
      "Epoch 609 -> loss: 0.00016119438805617392\n",
      "Epoch 610 -> loss: 0.00016086033429019153\n",
      "Epoch 611 -> loss: 0.000160583556862548\n",
      "Epoch 612 -> loss: 0.00016030683764256537\n",
      "Epoch 613 -> loss: 0.00016000375035218894\n",
      "Epoch 614 -> loss: 0.0001597079390194267\n",
      "Epoch 615 -> loss: 0.0001594573986949399\n",
      "Epoch 616 -> loss: 0.00015912807430140674\n",
      "Epoch 617 -> loss: 0.00015883223386481404\n",
      "Epoch 618 -> loss: 0.00015854112280067056\n",
      "Epoch 619 -> loss: 0.00015827752940822393\n",
      "Epoch 620 -> loss: 0.00015795652871020138\n",
      "Epoch 621 -> loss: 0.00015765953867230564\n",
      "Epoch 622 -> loss: 0.00015740303206257522\n",
      "Epoch 623 -> loss: 0.00015713818720541894\n",
      "Epoch 624 -> loss: 0.00015682322555221617\n",
      "Epoch 625 -> loss: 0.00015651779540348798\n",
      "Epoch 626 -> loss: 0.00015625778178218752\n",
      "Epoch 627 -> loss: 0.00015597857418470085\n",
      "Epoch 628 -> loss: 0.00015570539108011872\n",
      "Epoch 629 -> loss: 0.00015541787433903664\n",
      "Epoch 630 -> loss: 0.00015512201935052872\n",
      "Epoch 631 -> loss: 0.00015487032942473888\n",
      "Epoch 632 -> loss: 0.00015456970140803605\n",
      "Epoch 633 -> loss: 0.0001542953250464052\n",
      "Epoch 634 -> loss: 0.00015401137352455407\n",
      "Epoch 635 -> loss: 0.00015373816131614149\n",
      "Epoch 636 -> loss: 0.00015347098815254867\n",
      "Epoch 637 -> loss: 0.00015317268844228238\n",
      "Epoch 638 -> loss: 0.0001528995344415307\n",
      "Epoch 639 -> loss: 0.00015267886919900775\n",
      "Epoch 640 -> loss: 0.00015235907630994916\n",
      "Epoch 641 -> loss: 0.00015209909179247916\n",
      "Epoch 642 -> loss: 0.0001518437929917127\n",
      "Epoch 643 -> loss: 0.00015155987057369202\n",
      "Epoch 644 -> loss: 0.00015124368655961007\n",
      "Epoch 645 -> loss: 0.0001510158326709643\n",
      "Epoch 646 -> loss: 0.00015077013813424855\n",
      "Epoch 647 -> loss: 0.0001504455867689103\n",
      "Epoch 648 -> loss: 0.00015022611478343606\n",
      "Epoch 649 -> loss: 0.00014994695084169507\n",
      "Epoch 650 -> loss: 0.00014966783055569977\n",
      "Epoch 651 -> loss: 0.0001493588206358254\n",
      "Epoch 652 -> loss: 0.0001491727598477155\n",
      "Epoch 653 -> loss: 0.00014886495773680508\n",
      "Epoch 654 -> loss: 0.00014859771181363612\n",
      "Epoch 655 -> loss: 0.00014834965986665338\n",
      "Epoch 656 -> loss: 0.0001480799837736413\n",
      "Epoch 657 -> loss: 0.0001478056365158409\n",
      "Epoch 658 -> loss: 0.00014757185999769717\n",
      "Epoch 659 -> loss: 0.0001472867006668821\n",
      "Epoch 660 -> loss: 0.00014702428597956896\n",
      "Epoch 661 -> loss: 0.0001467761176172644\n",
      "Epoch 662 -> loss: 0.00014655785344075412\n",
      "Epoch 663 -> loss: 0.0001462548243580386\n",
      "Epoch 664 -> loss: 0.00014604369061999023\n",
      "Epoch 665 -> loss: 0.00014572634245269\n",
      "Epoch 666 -> loss: 0.0001455044694012031\n",
      "Epoch 667 -> loss: 0.00014524442667607218\n",
      "Epoch 668 -> loss: 0.00014503802231047302\n",
      "Epoch 669 -> loss: 0.00014478276716545224\n",
      "Epoch 670 -> loss: 0.00014451914466917515\n",
      "Epoch 671 -> loss: 0.00014423520769923925\n",
      "Epoch 672 -> loss: 0.00014405509864445776\n",
      "Epoch 673 -> loss: 0.00014379268395714462\n",
      "Epoch 674 -> loss: 0.00014349199773278087\n",
      "Epoch 675 -> loss: 0.00014327971439342946\n",
      "Epoch 676 -> loss: 0.00014306975936051458\n",
      "Epoch 677 -> loss: 0.00014287415251601487\n",
      "Epoch 678 -> loss: 0.00014254607958719134\n",
      "Epoch 679 -> loss: 0.00014232417743187398\n",
      "Epoch 680 -> loss: 0.000142078468343243\n",
      "Epoch 681 -> loss: 0.00014184463361743838\n",
      "Epoch 682 -> loss: 0.00014158939302433282\n",
      "Epoch 683 -> loss: 0.00014139374252408743\n",
      "Epoch 684 -> loss: 0.00014114803343545645\n",
      "Epoch 685 -> loss: 0.0001409428659826517\n",
      "Epoch 686 -> loss: 0.00014065530558582395\n",
      "Epoch 687 -> loss: 0.00014045376155991107\n",
      "Epoch 688 -> loss: 0.00014020797971170396\n",
      "Epoch 689 -> loss: 0.0001400112232659012\n",
      "Epoch 690 -> loss: 0.0001397463638568297\n",
      "Epoch 691 -> loss: 0.0001395197177771479\n",
      "Epoch 692 -> loss: 0.0001392930862493813\n",
      "Epoch 693 -> loss: 0.0001390712131978944\n",
      "Epoch 694 -> loss: 0.00013882189523428679\n",
      "Epoch 695 -> loss: 0.00013857614248991013\n",
      "Epoch 696 -> loss: 0.00013839246821589768\n",
      "Epoch 697 -> loss: 0.00013815266720484942\n",
      "Epoch 698 -> loss: 0.00013789146032650024\n",
      "Epoch 699 -> loss: 0.0001376886502839625\n",
      "Epoch 700 -> loss: 0.00013745961769018322\n",
      "Epoch 701 -> loss: 0.00013727832993026823\n",
      "Epoch 702 -> loss: 0.0001369848323520273\n",
      "Epoch 703 -> loss: 0.0001367964141536504\n",
      "Epoch 704 -> loss: 0.000136529139126651\n",
      "Epoch 705 -> loss: 0.000136345493956469\n",
      "Epoch 706 -> loss: 0.00013609020970761776\n",
      "Epoch 707 -> loss: 0.0001359124726150185\n",
      "Epoch 708 -> loss: 0.0001356476714136079\n",
      "Epoch 709 -> loss: 0.00013543768727686256\n",
      "Epoch 710 -> loss: 0.0001352182443952188\n",
      "Epoch 711 -> loss: 0.00013500590284820646\n",
      "Epoch 712 -> loss: 0.00013479597691912204\n",
      "Epoch 713 -> loss: 0.00013454780855681747\n",
      "Epoch 714 -> loss: 0.00013437369489111006\n",
      "Epoch 715 -> loss: 0.0001341267634415999\n",
      "Epoch 716 -> loss: 0.00013392514665611088\n",
      "Epoch 717 -> loss: 0.0001337104622507468\n",
      "Epoch 718 -> loss: 0.00013353511167224497\n",
      "Epoch 719 -> loss: 0.00013329890498425812\n",
      "Epoch 720 -> loss: 0.00013307947665452957\n",
      "Epoch 721 -> loss: 0.00013285994646139443\n",
      "Epoch 722 -> loss: 0.00013267388567328453\n",
      "Epoch 723 -> loss: 0.0001324400509474799\n",
      "Epoch 724 -> loss: 0.00013225401926320046\n",
      "Epoch 725 -> loss: 0.00013203213165979832\n",
      "Epoch 726 -> loss: 0.00013180545647628605\n",
      "Epoch 727 -> loss: 0.00013160270464140922\n",
      "Epoch 728 -> loss: 0.00013140944065526128\n",
      "Epoch 729 -> loss: 0.00013118401693645865\n",
      "Epoch 730 -> loss: 0.00013100987416692078\n",
      "Epoch 731 -> loss: 0.0001308094651903957\n",
      "Epoch 732 -> loss: 0.00013057806063443422\n",
      "Epoch 733 -> loss: 0.00013041344936937094\n",
      "Epoch 734 -> loss: 0.0001301939773838967\n",
      "Epoch 735 -> loss: 0.00013001983461435884\n",
      "Epoch 736 -> loss: 0.0001297812268603593\n",
      "Epoch 737 -> loss: 0.00012957131548319012\n",
      "Epoch 738 -> loss: 0.00012938285362906754\n",
      "Epoch 739 -> loss: 0.0001291920052608475\n",
      "Epoch 740 -> loss: 0.00012896774569526315\n",
      "Epoch 741 -> loss: 0.00012878286361228675\n",
      "Epoch 742 -> loss: 0.00012858127593062818\n",
      "Epoch 743 -> loss: 0.00012840356794185936\n",
      "Epoch 744 -> loss: 0.00012818642426282167\n",
      "Epoch 745 -> loss: 0.00012798966781701893\n",
      "Epoch 746 -> loss: 0.0001278059498872608\n",
      "Epoch 747 -> loss: 0.00012760555546265095\n",
      "Epoch 748 -> loss: 0.00012739803059957922\n",
      "Epoch 749 -> loss: 0.00012724057887680829\n",
      "Epoch 750 -> loss: 0.0001270258944714442\n",
      "Epoch 751 -> loss: 0.00012686963600572199\n",
      "Epoch 752 -> loss: 0.00012667760893236846\n",
      "Epoch 753 -> loss: 0.00012647600669879466\n",
      "Epoch 754 -> loss: 0.00012626132229343057\n",
      "Epoch 755 -> loss: 0.00012611577403731644\n",
      "Epoch 756 -> loss: 0.0001259154232684523\n",
      "Epoch 757 -> loss: 0.00012571383558679372\n",
      "Epoch 758 -> loss: 0.00012551937834359705\n",
      "Epoch 759 -> loss: 0.00012535002315416932\n",
      "Epoch 760 -> loss: 0.00012516631977632642\n",
      "Epoch 761 -> loss: 0.00012497787247411907\n",
      "Epoch 762 -> loss: 0.00012479416909627616\n",
      "Epoch 763 -> loss: 0.00012464750034268945\n",
      "Epoch 764 -> loss: 0.00012443512969184667\n",
      "Epoch 765 -> loss: 0.00012424671149346977\n",
      "Epoch 766 -> loss: 0.00012407968461047858\n",
      "Epoch 767 -> loss: 0.00012388885079417378\n",
      "Epoch 768 -> loss: 0.00012371948105283082\n",
      "Epoch 769 -> loss: 0.0001235334057128057\n",
      "Epoch 770 -> loss: 0.0001233580696862191\n",
      "Epoch 771 -> loss: 0.000123186269775033\n",
      "Epoch 772 -> loss: 0.00012300023809075356\n",
      "Epoch 773 -> loss: 0.00012281653471291065\n",
      "Epoch 774 -> loss: 0.00012262328527867794\n",
      "Epoch 775 -> loss: 0.00012245155812706798\n",
      "Epoch 776 -> loss: 0.00012228218838572502\n",
      "Epoch 777 -> loss: 0.0001221080165123567\n",
      "Epoch 778 -> loss: 0.00012191122368676588\n",
      "Epoch 779 -> loss: 0.0001217609315062873\n",
      "Epoch 780 -> loss: 0.00012157959281466901\n",
      "Epoch 781 -> loss: 0.00012142218474764377\n",
      "Epoch 782 -> loss: 0.000121224184113089\n",
      "Epoch 783 -> loss: 0.00012104643974453211\n",
      "Epoch 784 -> loss: 0.00012087946379324421\n",
      "Epoch 785 -> loss: 0.00012070291995769367\n",
      "Epoch 786 -> loss: 0.00012055264232913032\n",
      "Epoch 787 -> loss: 0.00012033551320200786\n",
      "Epoch 788 -> loss: 0.00012019959831377491\n",
      "Epoch 789 -> loss: 0.00012001110007986426\n",
      "Epoch 790 -> loss: 0.00011985367746092379\n",
      "Epoch 791 -> loss: 0.00011967951286351308\n",
      "Epoch 792 -> loss: 0.00011949108738917857\n",
      "Epoch 793 -> loss: 0.00011932168126804754\n",
      "Epoch 794 -> loss: 0.000119145137432497\n",
      "Epoch 795 -> loss: 0.0001189793401863426\n",
      "Epoch 796 -> loss: 0.00011881237878696993\n",
      "Epoch 797 -> loss: 0.000118657284474466\n",
      "Epoch 798 -> loss: 0.00011847481073345989\n",
      "Epoch 799 -> loss: 0.00011831615847768262\n",
      "Epoch 800 -> loss: 0.0001181611223728396\n",
      "Epoch 801 -> loss: 0.00011795353930210695\n",
      "Epoch 802 -> loss: 0.00011781638022512197\n",
      "Epoch 803 -> loss: 0.00011765419912990183\n",
      "Epoch 804 -> loss: 0.00011747285316232592\n",
      "Epoch 805 -> loss: 0.00011730588448699564\n",
      "Epoch 806 -> loss: 0.00011712459672708064\n",
      "Epoch 807 -> loss: 0.00011697193258441985\n",
      "Epoch 808 -> loss: 0.00011681207979563624\n",
      "Epoch 809 -> loss: 0.00011665701458696276\n",
      "Epoch 810 -> loss: 0.0001164661516668275\n",
      "Epoch 811 -> loss: 0.00011632542737061158\n",
      "Epoch 812 -> loss: 0.00011615129187703133\n",
      "Epoch 813 -> loss: 0.00011600816651480272\n",
      "Epoch 814 -> loss: 0.0001158411760115996\n",
      "Epoch 815 -> loss: 0.00011566225293790922\n",
      "Epoch 816 -> loss: 0.00011550957424333319\n",
      "Epoch 817 -> loss: 0.00011535213707247749\n",
      "Epoch 818 -> loss: 0.00011518037354107946\n",
      "Epoch 819 -> loss: 0.00011502293637022376\n",
      "Epoch 820 -> loss: 0.00011487263691378757\n",
      "Epoch 821 -> loss: 0.00011472235200926661\n",
      "Epoch 822 -> loss: 0.00011455060302978382\n",
      "Epoch 823 -> loss: 0.00011440268281148747\n",
      "Epoch 824 -> loss: 0.00011424047261243686\n",
      "Epoch 825 -> loss: 0.00011412837920943275\n",
      "Epoch 826 -> loss: 0.00011392081069061533\n",
      "Epoch 827 -> loss: 0.00011374668247299269\n",
      "Epoch 828 -> loss: 0.00011361308861523867\n",
      "Epoch 829 -> loss: 0.00011345086386427283\n",
      "Epoch 830 -> loss: 0.00011333400470903143\n",
      "Epoch 831 -> loss: 0.00011314671428408474\n",
      "Epoch 832 -> loss: 0.00011299405014142394\n",
      "Epoch 833 -> loss: 0.00011286047811154276\n",
      "Epoch 834 -> loss: 0.0001126791539718397\n",
      "Epoch 835 -> loss: 0.00011252650438109413\n",
      "Epoch 836 -> loss: 0.00011236307182116434\n",
      "Epoch 837 -> loss: 0.00011222470493521541\n",
      "Epoch 838 -> loss: 0.00011208878277102485\n",
      "Epoch 839 -> loss: 0.00011191699741175398\n",
      "Epoch 840 -> loss: 0.0001117571591748856\n",
      "Epoch 841 -> loss: 0.00011164030729560181\n",
      "Epoch 842 -> loss: 0.00011145420285174623\n",
      "Epoch 843 -> loss: 0.00011132063082186505\n",
      "Epoch 844 -> loss: 0.00011116079986095428\n",
      "Epoch 845 -> loss: 0.00011099380935775116\n",
      "Epoch 846 -> loss: 0.0001108578362618573\n",
      "Epoch 847 -> loss: 0.00011069561878684908\n",
      "Epoch 848 -> loss: 0.00011055487266276032\n",
      "Epoch 849 -> loss: 0.00011040461686206982\n",
      "Epoch 850 -> loss: 0.00011025551793863997\n",
      "Epoch 851 -> loss: 0.00011011713650077581\n",
      "Epoch 852 -> loss: 0.00010993824253091589\n",
      "Epoch 853 -> loss: 0.00010980226215906441\n",
      "Epoch 854 -> loss: 0.00010966511035803705\n",
      "Epoch 855 -> loss: 0.00010951003787340596\n",
      "Epoch 856 -> loss: 0.00010936454782495275\n",
      "Epoch 857 -> loss: 0.0001091999511118047\n",
      "Epoch 858 -> loss: 0.0001090782752726227\n",
      "Epoch 859 -> loss: 0.00010892800492001697\n",
      "Epoch 860 -> loss: 0.00010877534805331379\n",
      "Epoch 861 -> loss: 0.0001086393604055047\n",
      "Epoch 862 -> loss: 0.00010848432430066168\n",
      "Epoch 863 -> loss: 0.00010835548891918734\n",
      "Epoch 864 -> loss: 0.00010819330782396719\n",
      "Epoch 865 -> loss: 0.00010802508768392727\n",
      "Epoch 866 -> loss: 0.00010788434883579612\n",
      "Epoch 867 -> loss: 0.00010775795817608014\n",
      "Epoch 868 -> loss: 0.00010760525037767366\n",
      "Epoch 869 -> loss: 0.00010744304745458066\n",
      "Epoch 870 -> loss: 0.00010731424845289439\n",
      "Epoch 871 -> loss: 0.00010718304110923782\n",
      "Epoch 872 -> loss: 0.00010702321742428467\n",
      "Epoch 873 -> loss: 0.00010690633644117042\n",
      "Epoch 874 -> loss: 0.00010672740609152243\n",
      "Epoch 875 -> loss: 0.00010661053966032341\n",
      "Epoch 876 -> loss: 0.00010645307338563725\n",
      "Epoch 877 -> loss: 0.00010632309567881748\n",
      "Epoch 878 -> loss: 0.00010617519728839397\n",
      "Epoch 879 -> loss: 0.0001060439899447374\n",
      "Epoch 880 -> loss: 0.000105891318526119\n",
      "Epoch 881 -> loss: 0.00010573385952739045\n",
      "Epoch 882 -> loss: 0.00010561939416220412\n",
      "Epoch 883 -> loss: 0.00010545479017309844\n",
      "Epoch 884 -> loss: 0.00010530689178267494\n",
      "Epoch 885 -> loss: 0.00010519119678065181\n",
      "Epoch 886 -> loss: 0.00010505403770366684\n",
      "Epoch 887 -> loss: 0.000104910897789523\n",
      "Epoch 888 -> loss: 0.00010478448530193418\n",
      "Epoch 889 -> loss: 0.0001046151082846336\n",
      "Epoch 890 -> loss: 0.00010450658737681806\n",
      "Epoch 891 -> loss: 0.00010435152216814458\n",
      "Epoch 892 -> loss: 0.00010424181527923793\n",
      "Epoch 893 -> loss: 0.00010407242371002212\n",
      "Epoch 894 -> loss: 0.0001039460112224333\n",
      "Epoch 895 -> loss: 0.0001038028858602047\n",
      "Epoch 896 -> loss: 0.00010368123184889555\n",
      "Epoch 897 -> loss: 0.00010354766709497198\n",
      "Epoch 898 -> loss: 0.00010340930020902306\n",
      "Epoch 899 -> loss: 0.00010327570635126904\n",
      "Epoch 900 -> loss: 0.00010315408871974796\n",
      "Epoch 901 -> loss: 0.00010299900168320164\n",
      "Epoch 902 -> loss: 0.00010285826283507049\n",
      "Epoch 903 -> loss: 0.00010273423686157912\n",
      "Epoch 904 -> loss: 0.00010260541603202\n",
      "Epoch 905 -> loss: 0.0001024766024784185\n",
      "Epoch 906 -> loss: 0.0001023370714392513\n",
      "Epoch 907 -> loss: 0.00010221659613307565\n",
      "Epoch 908 -> loss: 0.0001020794443320483\n",
      "Epoch 909 -> loss: 0.00010193872731178999\n",
      "Epoch 910 -> loss: 0.00010182659752899781\n",
      "Epoch 911 -> loss: 0.00010170016321353614\n",
      "Epoch 912 -> loss: 0.00010158807708648965\n",
      "Epoch 913 -> loss: 0.00010144255793420598\n",
      "Epoch 914 -> loss: 0.00010133282921742648\n",
      "Epoch 915 -> loss: 0.00010119209036929533\n",
      "Epoch 916 -> loss: 0.00010108237620443106\n",
      "Epoch 917 -> loss: 0.00010093685705214739\n",
      "Epoch 918 -> loss: 0.0001008223625831306\n",
      "Epoch 919 -> loss: 0.00010069117706734687\n",
      "Epoch 920 -> loss: 0.00010056953033199534\n",
      "Epoch 921 -> loss: 0.0001004216173896566\n",
      "Epoch 922 -> loss: 0.00010031191050074995\n",
      "Epoch 923 -> loss: 0.00010018785542342812\n",
      "Epoch 924 -> loss: 0.000100056677183602\n",
      "Epoch 925 -> loss: 9.992309787776321e-05\n",
      "Epoch 926 -> loss: 9.981097537092865e-05\n",
      "Epoch 927 -> loss: 9.968216909328476e-05\n",
      "Epoch 928 -> loss: 9.956290887203068e-05\n",
      "Epoch 929 -> loss: 9.945795318344608e-05\n",
      "Epoch 930 -> loss: 9.932675311574712e-05\n",
      "Epoch 931 -> loss: 9.92003406281583e-05\n",
      "Epoch 932 -> loss: 9.907747153192759e-05\n",
      "Epoch 933 -> loss: 9.895346011035144e-05\n",
      "Epoch 934 -> loss: 9.882702579488978e-05\n",
      "Epoch 935 -> loss: 9.87173116300255e-05\n",
      "Epoch 936 -> loss: 9.857656550593674e-05\n",
      "Epoch 937 -> loss: 9.845731256064028e-05\n",
      "Epoch 938 -> loss: 9.83451827778481e-05\n",
      "Epoch 939 -> loss: 9.823069558478892e-05\n",
      "Epoch 940 -> loss: 9.811619383981451e-05\n",
      "Epoch 941 -> loss: 9.79718824964948e-05\n",
      "Epoch 942 -> loss: 9.786216105567291e-05\n",
      "Epoch 943 -> loss: 9.774528007255867e-05\n",
      "Epoch 944 -> loss: 9.762842091731727e-05\n",
      "Epoch 945 -> loss: 9.750794561114162e-05\n",
      "Epoch 946 -> loss: 9.739106462802738e-05\n",
      "Epoch 947 -> loss: 9.72670313785784e-05\n",
      "Epoch 948 -> loss: 9.714298357721418e-05\n",
      "Epoch 949 -> loss: 9.700703230919316e-05\n",
      "Epoch 950 -> loss: 9.69139946391806e-05\n",
      "Epoch 951 -> loss: 9.679235517978668e-05\n",
      "Epoch 952 -> loss: 9.667547419667244e-05\n",
      "Epoch 953 -> loss: 9.655621397541836e-05\n",
      "Epoch 954 -> loss: 9.643575322115794e-05\n",
      "Epoch 955 -> loss: 9.632125147618353e-05\n",
      "Epoch 956 -> loss: 9.621154458727688e-05\n",
      "Epoch 957 -> loss: 9.607795072952285e-05\n",
      "Epoch 958 -> loss: 9.595869050826877e-05\n",
      "Epoch 959 -> loss: 9.584896179148927e-05\n",
      "Epoch 960 -> loss: 9.573208080837503e-05\n",
      "Epoch 961 -> loss: 9.563906496623531e-05\n",
      "Epoch 962 -> loss: 9.549832611810416e-05\n",
      "Epoch 963 -> loss: 9.538860467728227e-05\n",
      "Epoch 964 -> loss: 9.527887596050277e-05\n",
      "Epoch 965 -> loss: 9.518108709016815e-05\n",
      "Epoch 966 -> loss: 9.50737448874861e-05\n",
      "Epoch 967 -> loss: 9.494494588579983e-05\n",
      "Epoch 968 -> loss: 9.48423839872703e-05\n",
      "Epoch 969 -> loss: 9.473027603235096e-05\n",
      "Epoch 970 -> loss: 9.460624278290197e-05\n",
      "Epoch 971 -> loss: 9.44977073231712e-05\n",
      "Epoch 972 -> loss: 9.437128755962476e-05\n",
      "Epoch 973 -> loss: 9.428303746972233e-05\n",
      "Epoch 974 -> loss: 9.415423119207844e-05\n",
      "Epoch 975 -> loss: 9.40468889893964e-05\n",
      "Epoch 976 -> loss: 9.393716754857451e-05\n",
      "Epoch 977 -> loss: 9.382745338371024e-05\n",
      "Epoch 978 -> loss: 9.371296619065106e-05\n",
      "Epoch 979 -> loss: 9.361278353026137e-05\n",
      "Epoch 980 -> loss: 9.34911222429946e-05\n",
      "Epoch 981 -> loss: 9.341243276139721e-05\n",
      "Epoch 982 -> loss: 9.32788461796008e-05\n",
      "Epoch 983 -> loss: 9.317627700511366e-05\n",
      "Epoch 984 -> loss: 9.307609434472397e-05\n",
      "Epoch 985 -> loss: 9.295445488533005e-05\n",
      "Epoch 986 -> loss: 9.283518011216074e-05\n",
      "Epoch 987 -> loss: 9.273977775592357e-05\n",
      "Epoch 988 -> loss: 9.261931700166315e-05\n",
      "Epoch 989 -> loss: 9.253345342585817e-05\n",
      "Epoch 990 -> loss: 9.242611122317612e-05\n",
      "Epoch 991 -> loss: 9.232832962879911e-05\n",
      "Epoch 992 -> loss: 9.220786887453869e-05\n",
      "Epoch 993 -> loss: 9.211246651830152e-05\n",
      "Epoch 994 -> loss: 9.199080523103476e-05\n",
      "Epoch 995 -> loss: 9.190494165522978e-05\n",
      "Epoch 996 -> loss: 9.180716006085277e-05\n",
      "Epoch 997 -> loss: 9.16950375540182e-05\n",
      "Epoch 998 -> loss: 9.159008914139122e-05\n",
      "Epoch 999 -> loss: 9.148274693870917e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.95500004, dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessing\n",
    "dataset_path = \"../data/mfeat-pix\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "img_shape = (16, 15)\n",
    "\n",
    "data = preprocessing.load_data(dataset_path)\n",
    "\n",
    "num_classes = 10\n",
    "samples_per_class = 200\n",
    "train_samples_per_class = 100\n",
    "test_samples_per_class = 100\n",
    "\n",
    "train_data, test_data = preprocessing.split_data(data, num_classes, samples_per_class, train_samples_per_class)\n",
    "\n",
    "\n",
    "train_labels, test_labels = preprocessing.create_labels(num_classes, train_samples_per_class, test_samples_per_class)\n",
    "\n",
    "\n",
    "\n",
    "architecture = [train_data.shape[1], 128, 128, 128, 128, 128, 128, num_classes]\n",
    "\n",
    "print(architecture)\n",
    "params = neuron_initialization(architecture)\n",
    "\n",
    "param_trained, history = classification_train(params, cross_entropy_loss, train_data, train_labels, epochs=1000, batch_size = 50)\n",
    "\n",
    "accuracy(param_trained, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(params, train_data, train_labels, test_data, test_labels):\n",
    "    alphas = [0.001, 0.0005, 0.0001]\n",
    "    lasso_values = [0.01, 0.001, 0.0001]\n",
    "    ridge_values = [0.1, 0.01, 0.001] \n",
    "    batch_sizes = [50, 100]\n",
    "\n",
    "    best_acc = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for alpha, lambda_lasso, lambda_ridge, batch_size in itertools.product(alphas, lasso_values, ridge_values, batch_sizes):\n",
    "        print(f\"\\nTesting con: alpha={alpha}, Lasso={lambda_lasso}, Ridge={lambda_ridge}, batch_size={batch_size}\")\n",
    "\n",
    "        params = neuron_initialization(architecture)\n",
    "\n",
    "        trained_params, history = classification_train_2(\n",
    "            params, cross_entropy_loss, train_data, train_labels,\n",
    "            epochs=500, batch_size=batch_size, optimizer=\"adam\",\n",
    "            alpha=alpha, lambda_lasso=lambda_lasso, lambda_ridge=lambda_ridge\n",
    "        )\n",
    "\n",
    "        test_acc = accuracy(trained_params, test_data, test_labels)\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_params = {'alpha': alpha, 'lambda_lasso': lambda_lasso, 'lambda_ridge': lambda_ridge, 'batch_size': batch_size}\n",
    "\n",
    "    print(\"\\nMiglior combinazione trovata:\")\n",
    "    print(f\"alpha={best_params['alpha']}, lambda_lasso={best_params['lambda_lasso']}, lambda_ridge={best_params['lambda_ridge']}, batch_size={best_params['batch_size']}\")\n",
    "    print(f\"Best Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.95100003, dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(param_trained, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
